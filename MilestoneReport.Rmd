---
title: "Coursera Data Science Capstone Project: Milestone report"
author: "Oscar Rodriguez"
date: "29 December 2015"
output: html_document
---
# Introduction

The goal of this report is summarising the progress achieved in working with the data and  create the prediction algorithm.Tables and plots are used to illustrate important summaries of the data set.

The motivation for this report is to: 

1. Demonstrate that you've downloaded the data and have successfully loaded it in.
2. Create a basic report of summary statistics about the data sets.
3. Report the findings amassed so far.
4. Get feedback the reported next steps for creating a prediction algorithm and Shiny app.

# Getting and cleaning the data
We download three files to be used as the corpora in English language (US) for building the next word prediction model. The Unix command `wc` (for word count), provides a basic summary on the files, presented in the table below.
 
 File | Lines | Words | bytes 
------|------| ------|------
en_US.blogs.txt | 899288| 37334690 | 210160014 
en_US.news.txt | 1010242 | 34372720 | 205811889 
en_US.twitter.txt| 2360148 | 30374206 | 167105338 

## Profanity filtering
Next, we proceed to clean the data. In this case, we start by applying a profanity filter by deleting all entries containing at least one of the words in a list of 21 common English profane words. 
The `grep` (with option `-v` to exclude matches) achieves just that:
```
cat en_US.twitter.txt  | grep -vf en_profane_wordlist.txt  > en_US.twitter.pf.txt
```
As summarized in the table below, the number of filtered lines is negligible in relative terms, and thus shall not impact the prediction model.

File | Lines | Words | bytes |% lines removed
------|------| ------|------|------
en_US.blogs.pf.txt |  891330 | 36666585 | 206506505 | `r round((1-891330/899288)*100,2)`
en_US.news.pf.txt | 1009850 | 34349793 | 205679018 | `r round((1-1009850/1010242)*100,2)`
en_US.twitter.pf.txt | 2309348 | 29638990 | 163283024 | `r round((1-2309348/2360148)*100,2)` 

Note that tiwtter has higher number of entries containing profane words, followed by blogs and finally news.

## Tokenization

We perform word tokenization of the resulting files. At this stage the objective is obtaining the counts of distinct words in each file. For this task we made the following decisions:

1. converting all words to lower case, which should simplify analysis, and reduce the number of tokens, thus simplifying the overall model.
2. removing all punctuation from the dataset. New lines replace periods, this will be important for n-gram extraction. Note that this will not deal with items containing punctuation such as URLs or abbreviations, which we assume will not be significant in the overall model.
3. removing all tokens containing non alphabetic characters (this will be our definition of word or unigram).

### Unigram extraction
We apply the rules described above for unigram extraction. The UNIX command below generate a file with a unigram and  its associated count in the source file. 

```
cat en_US.twitter.pf.txt | tr '[:upper:]' '[:lower:]' | tr -d '[:punct:]' | tr ' ' '\n' |  tr -d ' ' | grep  [a-z] | grep -v [0-9]| sort > twitter_unigrams.txt
cat twitter_unigrams.txt | uniq -c | sort -rn > twitter_unigrams_counts.txt
```


## Bigram extraction

For bigram extraction we paste the rows in two files containing the ordered unigrams, after creating a 1 row displacement in the second file.
This technique is followed for successive extractions
```
cat en_US.twitter.txt | tr '[:upper:]' '[:lower:]' | tr -d '[:punct:]' | sed 's/,//' | sed G | tr ' ' '\n' > tmp.txt
  tail -n+2 tmp.txt > tmp2.txt  
  paste -d ',' tmp.txt tmp2.txt | grep -v -e "^," | grep -v -e ",$"|grep  -e "^[a-z]*,[a-z]*$"  | sort | uniq -c | sort -rn > twitter_bigrams_counts.txt
```

Higher order n-grams are extracted in an analog manner.
 
## Exploratory data analysis

## Unigrams


```{r read_unigrams, echo=FALSE, cache=TRUE}

read_ngrams<-function(myfile)
{
  mytable<-read.table(myfile)
  names(mytable)<-c("count", "ngram")
  mytable$cumfreq<- cumsum(mytable$count/sum(mytable$count))
  mytable
}

getquantiles<-function(mydf)
{
  quantile(mydf$count,  probs = c (50,75, 90)/100)
}

twitter_uni <- read_ngrams("~/Documents/Oscar/CourseraCapstone/en_US/twitter_unigrams_counts.txt")
twitter_q <- getquantiles(twitter_uni)

blogs_uni <- read_ngrams("~/Documents/Oscar/CourseraCapstone/en_US/blogs_unigrams_counts.txt")
blogs_q <- getquantiles(blogs_uni)

news_uni <- read_ngrams("~/Documents/Oscar/CourseraCapstone/en_US/news_unigrams_counts.txt")
news_q <- getquantiles(news_uni)
```

By inspecting the first few lines of the unigram count table, it can be noted that the most frequent unigrams are stopwords.

```{r head_unigrams,echo=FALSE}
head(twitter_uni)
```

Conversely, the last lines are mostly typos, which is to be expected of unigrams occurring exactly once in such a sizeable corpus.
Hence these unigrams can be safely disregarded from the prediction model.
```{r tail_unigrams,echo=FALSE}
tail(twitter_uni$ngram)
```

The  table below reports summary data on the unigram frequencies per source file.

File | distinct unigrams | total unigrams | 50% |75% |90% | Max
------|------| ------|------|------|------|------
blogs |  `r length(blogs_uni$unigram)` | `r sum(blogs_uni$count)` | `r blogs_q[1]` | `r blogs_q[2]` | `r blogs_q[3]` | `r max(blogs_uni$count)`
news |  `r length(news_uni$unigram)` | `r sum(news_uni$count)`  | `r news_q[1]` | `r news_q[2]` | `r news_q[3]` | `r max(news_uni$count)`
twitter |  `r length(twitter_uni$unigram)` | `r sum(twitter_uni$count)`  | `r twitter_q[1]` | `r twitter_q[2]` | `r twitter_q[3]` | `r max(twitter_uni$count)`

Note that most unigrams have relatively low frequencies when compared to the maximum counts, and therefore, we will be able to achieve relatively good coverage of the unigram occurrences in a file while keeping a relatively small number of unigrams. 
We can analyse the coverage by ordering the unigrams by decreasing count and calculating the cumulative sum of probability density.

```{r plot_unigrams, echo=FALSE,cache=TRUE}

addprobabilitysegment<-function(myprobs,p, col)
{
  segments(which(myprobs > p)[1], p, which(myprobs > p)[1], 0, col= col, lwd = 2,lty="dashed")
  segments(which(myprobs > p)[1], p, 0, p, col= col, lwd = 2, lty = "dashed")
}

plotCumulativeProbability<-function(v,title)
{
  
  index<-which( v$cumfreq > 0.9)[1]
  plot(v$cumfreq, type='s', ylab = "cumulative probability" , main = title, log="x")
  addprobabilitysegment(v$cumfreq,0.5, "blue")
  addprobabilitysegment(v$cumfreq,0.9, "red")
}

par(mfrow=c(1,3))
plotCumulativeProbability(twitter_uni, "twitter")
plotCumulativeProbability(news_uni, "news")
plotCumulativeProbability(blogs_uni, "blogs")
```

The plots above depict the accumulated probability of covering a given unigram occurrence in a file when keeping the previous unigrams. The 50% coverage probability and 90% coverage probability (red) are represented.
Note that the x axis is to  logarithmic in order to improve the display of the above mention coverage points.
As an example, we can cover 50%  of the overall unigram occurrences in the blogs file by keeping the  `r round(which(blogs_uni$cumfreq>0.5)[1]/length(blogs_uni$cumfreq)*100,2)` % most frequent unigrams (or a total `r which(blogs_uni$cumfreq>0.5)[1]` of the `r length(blogs_uni$cumfreq)` unique unigrams in the source file) .

The table below summarizes this information.

File | count 50% coverage| percentage 50%  coverage| count  90% coverage | percentage  90% coverage
------|------| ------|------|------
blogs |  `r which(blogs_uni$cumfreq>0.5)[1]` | `r round(which(blogs_uni$cumfreq>0.5)[1]/length(blogs_uni$cumfreq)*100,2)` |  `r which(blogs_uni$cumfreq>0.9)[1]` | `r round(which(blogs_uni$cumfreq>0.9)[1]/length(blogs_uni$cumfreq)*100,2)`
news |  `r which(news_uni$cumfreq>0.5)[1]` | `r round(which(news_uni$cumfreq>0.5)[1]/length(news_uni$cumfreq)*100,2)` |  `r which(news_uni$cumfreq>0.9)[1]` | `r round(which(news_uni$cumfreq>0.9)[1]/length(news_uni$cumfreq)*100,2)`
twitter |  `r which(twitter_uni$cumfreq>0.5)[1]` | `r round(which(twitter_uni$cumfreq>0.5)[1]/length(twitter_uni$cumfreq)*100,2)` |  `r which(twitter_uni$cumfreq>0.9)[1]` | `r round(which(twitter_uni$cumfreq>0.9)[1]/length(twitter_uni$cumfreq)*100,2)`

## Bigrams
In this section, we perform the same analysis for the case of bigrams.
```{r read_bigrams, cache=TRUE, echo=FALSE}
twitter_bi <- read_ngrams("~/Documents/Oscar/CourseraCapstone/en_US/twitter_bigrams_counts.txt")
twitter_q <- getquantiles(twitter_bi)


blogs_bi <- read_ngrams("~/Documents/Oscar/CourseraCapstone/en_US/blogs_bigrams_counts.txt")
blogs_q <- getquantiles(blogs_bi)

news_bi <- read_ngrams("~/Documents/Oscar/CourseraCapstone/en_US/news_bigrams_counts.txt")
news_q <- getquantiles(news_bi)

```

File | distinct unigrams | total unigrams | 50% |75% |90% | Max
------|------| ------|------|------|------|------
blogs |  `r length(blogs_bi$bigram)` | `r sum(blogs_bi$count)` | `r blogs_q[1]` | `r blogs_q[2]` | `r blogs_q[3]` | `r max(blogs_bi$count)`
news |  `r length(news_bi$bigram)` | `r sum(news_bi$count)`  | `r news_q[1]` | `r news_q[2]` | `r news_q[3]` | `r max(news_bi$count)`
twitter |  `r length(twitter_bi$biigram)` | `r sum(twitter_bi$count)`  | `r twitter_q[1]` | `r twitter_q[2]` | `r twitter_q[3]` | `r max(twitter_bi$count)`

```{r plot_bigrams, echo=FALSE}
par(mfrow=c(1,3))
 plotCumulativeProbability(twitter_bi, "twitter")
 plotCumulativeProbability(news_bi, "news")
 plotCumulativeProbability(blogs_bi, "blogs")
```

File | count 50% coverage| percentage 50%  coverage| count  90% coverage | percentage  90% coverage
------|------| ------|------|------
blogs |  `r which(blogs_uni$cumfreq>0.5)[1]` | `r round(which(blogs_bi$cumfreq>0.5)[1]/length(blogs_bi$cumfreq)*100,2)` |  `r which(blogs_bi$cumfreq>0.9)[1]` | `r round(which(blogs_bi$cumfreq>0.9)[1]/length(blogs_bi$cumfreq)*100,2)`
news |  `r which(news_bi$cumfreq>0.5)[1]` | `r round(which(news_bi$cumfreq>0.5)[1]/length(news_bi$cumfreq)*100,2)` |  `r which(news_bi$cumfreq>0.9)[1]` | `r round(which(news_bi$cumfreq>0.9)[1]/length(news_bi$cumfreq)*100,2)`
twitter |  `r which(twitter_bi$cumfreq>0.5)[1]` | `r round(which(twitter_bi$cumfreq>0.5)[1]/length(twitter_bi$cumfreq)*100,2)` |  `r which(twitter_bi$cumfreq>0.9)[1]` | `r round(which(twitter_bi$cumfreq>0.9)[1]/length(twitter_bi$cumfreq)*100,2)`

Note that as there are more combinations of bigrams, the probability is spread among more distinct unigrams, and hence the need to keep a higher percentage of bigrams in order to reach the same coverage. 
It is expected that this trend will be kept in the case of higher order n-grams.

# Analysis over different n-grams

The figure below depicts the counts of distinct n-grams per source file. Note the increase in the number of distinct n-grams as "n" increases. 

```{r fig1, echo=FALSE}
file<-c("twitter","twitter","twitter","news","news","news","blogs","blogs", "blogs")
ngram<-c("unigram", "bigram", "trigram","unigram", "bigram", "trigram","unigram", "bigram", "trigram")
count<-c(531591,4472435,11342874,367316,5848784,16491184, 445405,5924522,17688365)
count_trend<-data.frame(file,ngram,count)
count_trend$ngram =factor(count_trend$ngram,levels(count_trend$ngram)[c(3,1,2)])
library(ggplot2)
ggplot(count_trend, aes(x = ngram, y = count, group = file, colour=file), log10="y")+  scale_y_continuous(breaks=c(1000000,5000000,10000000))+ geom_line()
```

# Conclusions

We introduced an effective  method for tokenization, profanity filtering andn-gram extraction from large files in the corpus. This method is tailored for the task of building a prediction model for the next word.
Secondly we performed exploratory data analysis over the extracted n-gram frequency tables.
We see that  as we increase the number of words providing context to the next work prediction model, the required memory to keep the n-grams will increase, and the prediction performance will decrease, as more n-grams are to be searched.
Also, as "n" increases, it is required to keep and increasing number of n-grams to achieve the same coverage.
In order to keep the performance acceptable, we need to study the trade-off between model accuracy and performance.
As a starting point we will use the coverage analysis from the previous section. 
We may keep for the initial model only the most frequent n-grams, such that each subset ensures 50% coverage of the three files combined.

# Next steps

* Build parametric back-off prediction model based on coverage analysis (e.g. start by 50% coverage)
* Estimate and compare accuracy of each model as we increase coverage, using a sample of the data, and other estimation techniques (e.g. perplexity)
* Estimate performance and memory requirements using Rprof(), gc(), object.size()
* optimize model data structure to increase performance and decrease memory requirements, for example by using a look up table for unigrams and storing n-grams as tuples of small integers.
